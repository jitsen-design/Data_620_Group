{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T15:13:16.188461Z",
     "start_time": "2020-06-24T15:13:16.152468Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import urllib\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from textblob import TextBlob\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse.linalg import svds\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import os\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import gensim\n",
    "import string\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T01:14:13.729290Z",
     "start_time": "2020-06-24T01:14:12.925976Z"
    }
   },
   "outputs": [],
   "source": [
    "pride = urllib.request.urlopen('https://www.gutenberg.org/files/1342/1342-0.txt').read()\n",
    "pride_lines = pride.decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T15:50:28.253515Z",
     "start_time": "2020-06-24T15:50:28.239339Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_chapters(book):\n",
    "    chapters = {}\n",
    "    i = 0\n",
    "    b = 0\n",
    "    for x in range(1, 62):\n",
    "        i = book.find(\"Chapter\", + b)\n",
    "        b = book.find(\"Chapter\", i + 1)\n",
    "        chapters['Chapter {}'.format(x)] = book[i:b]\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T16:11:57.336924Z",
     "start_time": "2020-06-24T16:11:57.318396Z"
    }
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') + ['mr',\n",
    "                                     'mrs',\n",
    "                                     'miss', \n",
    "                                     'say',\n",
    "                                     'have', \n",
    "                                     'might',\n",
    "                                     'thought',\n",
    "                                     'would', \n",
    "                                     'could', \n",
    "                                     'make', \n",
    "                                     'much',\n",
    "                                     'dear',\n",
    "                                     'must',\n",
    "                                     'know',\n",
    "                                     'one',\n",
    "                                     'good',\n",
    "                                     'every',\n",
    "                                     'towards',\n",
    "                                     'give']\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatize_word(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        tag = get_wordnet_pos(nltk.pos_tag([word])[0][1])\n",
    "        return lemmatizer.lemmatize(word, pos=tag)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T15:53:12.334559Z",
     "start_time": "2020-06-24T15:53:12.320228Z"
    }
   },
   "outputs": [],
   "source": [
    "def all_sentences(Object):\n",
    "    vocab = []\n",
    "    for line in sent_tokenize(Object):\n",
    "        line= re.sub('[%s]|Chapter' % re.escape(string.punctuation), '', line)\n",
    "        line = re.sub('[^a-zA-Z\\ ]', '', line)\n",
    "        line = line.lower()\n",
    "        line = line.split()\n",
    "        line = [lemmatize_word(x) for x in line if lemmatize_word(x) not in stop]\n",
    "        vocab.append(line)\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T16:33:23.728431Z",
     "start_time": "2020-06-24T16:33:23.708186Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_frequecies(corpus):\n",
    "    dictionary = corpora.Dictionary(corpus)\n",
    "    # You can check the mapping by caling 'token2id' attribute.\n",
    "    id_map = dictionary.token2id\n",
    "    count_map = dictionary.dfs\n",
    "    word_count = {}\n",
    "    for word, id_ in id_map.items():\n",
    "        word_count[word] = count_map[id_]\n",
    "    sorted_corpus = sorted([(x,y) \n",
    "                           for x,y in word_count.items()], \n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "    return sorted_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Keywords in Full Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T16:33:48.479764Z",
     "start_time": "2020-06-24T16:33:24.931025Z"
    }
   },
   "outputs": [],
   "source": [
    "full_corpus = all_sentences(pride_lines[2229:])\n",
    "frequencies = get_frequecies(full_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T16:33:50.785643Z",
     "start_time": "2020-06-24T16:33:50.777459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elizabeth', 613),\n",
       " ('darcy', 350),\n",
       " ('bennet', 314),\n",
       " ('well', 287),\n",
       " ('sister', 286),\n",
       " ('see', 278),\n",
       " ('go', 267),\n",
       " ('lady', 252),\n",
       " ('bingley', 250),\n",
       " ('jane', 245),\n",
       " ('come', 243),\n",
       " ('think', 227),\n",
       " ('time', 217),\n",
       " ('though', 215),\n",
       " ('never', 212),\n",
       " ('soon', 208),\n",
       " ('may', 192),\n",
       " ('little', 182),\n",
       " ('take', 182),\n",
       " ('great', 179)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T16:33:57.503145Z",
     "start_time": "2020-06-24T16:33:57.492853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5396"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Keywords in Each Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T16:12:48.422159Z",
     "start_time": "2020-06-24T16:12:25.590645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1 [('bennet', 5), ('see', 5), ('visit', 5), ('man', 4), ('single', 4)]\n",
      "Chapter 2 [('bennet', 8), ('bingley', 5), ('visit', 4), ('acquaintance', 4), ('though', 3)]\n",
      "Chapter 3 [('bingley', 15), ('dance', 14), ('bennet', 11), ('lady', 8), ('room', 8)]\n",
      "Chapter 4 [('bingley', 11), ('never', 6), ('like', 6), ('darcy', 6), ('sister', 5)]\n",
      "Chapter 5 [('lucas', 7), ('bennet', 6), ('think', 4), ('well', 4), ('like', 4)]\n",
      "Chapter 6 [('darcy', 12), ('though', 10), ('elizabeth', 10), ('dance', 10), ('well', 9)]\n",
      "Chapter 7 [('elizabeth', 14), ('bennet', 11), ('well', 11), ('go', 10), ('jane', 10)]\n",
      "Chapter 8 [('bingley', 20), ('sister', 14), ('elizabeth', 13), ('darcy', 10), ('lady', 7)]\n",
      "Chapter 9 [('bingley', 14), ('bennet', 9), ('country', 9), ('elizabeth', 8), ('mother', 8)]\n",
      "Chapter 10 [('bingley', 17), ('darcy', 16), ('elizabeth', 15), ('letter', 8), ('write', 8)]\n",
      "Chapter 11 [('darcy', 12), ('elizabeth', 11), ('bingley', 8), ('laugh', 6), ('hurst', 6)]\n",
      "Chapter 12 [('elizabeth', 6), ('jane', 6), ('bennet', 4), ('netherfield', 4), ('carriage', 3)]\n",
      "Chapter 13 [('bennet', 10), ('daughter', 7), ('letter', 7), ('collins', 7), ('family', 6)]\n",
      "Chapter 14 [('lady', 13), ('bennet', 10), ('catherine', 6), ('young', 6), ('little', 5)]\n",
      "Chapter 15 [('bennet', 9), ('collins', 7), ('see', 7), ('lady', 6), ('young', 6)]\n",
      "Chapter 16 [('elizabeth', 18), ('wickham', 17), ('darcy', 17), ('man', 11), ('lady', 10)]\n",
      "Chapter 17 [('elizabeth', 8), ('wickham', 7), ('jane', 6), ('dance', 6), ('even', 6)]\n",
      "Chapter 18 [('darcy', 30), ('elizabeth', 26), ('bingley', 19), ('though', 17), ('wickham', 15)]\n",
      "Chapter 19 [('collins', 10), ('may', 9), ('elizabeth', 8), ('honour', 7), ('assure', 6)]\n",
      "Chapter 20 [('bennet', 15), ('collins', 13), ('elizabeth', 9), ('lizzy', 7), ('come', 7)]\n",
      "Chapter 21 [('jane', 9), ('bingley', 9), ('elizabeth', 8), ('sister', 8), ('hope', 8)]\n",
      "Chapter 22 [('collins', 14), ('elizabeth', 10), ('charlotte', 10), ('lucas', 9), ('bennet', 6)]\n",
      "Chapter 23 [('bennet', 13), ('lucas', 10), ('elizabeth', 9), ('collins', 9), ('charlotte', 7)]\n",
      "Chapter 24 [('think', 14), ('jane', 11), ('elizabeth', 9), ('sister', 9), ('bennet', 7)]\n",
      "Chapter 25 [('gardiner', 9), ('sister', 8), ('jane', 7), ('elizabeth', 7), ('love', 6)]\n",
      "Chapter 26 [('elizabeth', 16), ('see', 11), ('well', 10), ('wish', 9), ('go', 7)]\n",
      "Chapter 27 [('elizabeth', 8), ('shall', 7), ('first', 5), ('girl', 5), ('little', 4)]\n",
      "Chapter 28 [('charlotte', 14), ('elizabeth', 13), ('collins', 10), ('lady', 10), ('well', 7)]\n",
      "Chapter 29 [('lady', 25), ('catherine', 20), ('collins', 15), ('elizabeth', 13), ('young', 10)]\n",
      "Chapter 30 [('collins', 11), ('elizabeth', 9), ('soon', 7), ('family', 5), ('go', 5)]\n",
      "Chapter 31 [('darcy', 13), ('lady', 12), ('elizabeth', 10), ('fitzwilliam', 9), ('colonel', 8)]\n",
      "Chapter 32 [('elizabeth', 9), ('friend', 9), ('collins', 8), ('darcy', 7), ('seem', 7)]\n",
      "Chapter 33 [('darcy', 15), ('bingley', 8), ('fitzwilliam', 7), ('suppose', 7), ('walk', 6)]\n",
      "Chapter 34 [('darcy', 9), ('elizabeth', 9), ('feeling', 8), ('room', 5), ('manner', 5)]\n",
      "Chapter 35 [('sister', 15), ('wickham', 10), ('father', 10), ('soon', 8), ('last', 8)]\n",
      "Chapter 36 [('darcy', 9), ('read', 7), ('letter', 6), ('wish', 6), ('never', 6)]\n",
      "Chapter 37 [('go', 9), ('catherine', 9), ('lady', 9), ('collins', 6), ('rosings', 5)]\n",
      "Chapter 38 [('elizabeth', 10), ('collins', 5), ('lady', 5), ('great', 5), ('kindness', 4)]\n",
      "Chapter 39 [('go', 12), ('lydia', 11), ('think', 10), ('elizabeth', 9), ('u', 8)]\n",
      "Chapter 40 [('jane', 8), ('well', 8), ('darcy', 7), ('sister', 7), ('elizabeth', 6)]\n",
      "Chapter 41 [('lydia', 20), ('elizabeth', 10), ('go', 8), ('brighton', 8), ('little', 7)]\n",
      "Chapter 42 [('see', 12), ('elizabeth', 10), ('gardiner', 9), ('little', 6), ('place', 6)]\n",
      "Chapter 43 [('elizabeth', 40), ('gardiner', 18), ('walk', 16), ('see', 15), ('darcy', 14)]\n",
      "Chapter 44 [('darcy', 16), ('elizabeth', 14), ('though', 12), ('see', 11), ('manner', 8)]\n",
      "Chapter 45 [('elizabeth', 17), ('darcy', 14), ('bingley', 10), ('house', 6), ('look', 6)]\n",
      "Chapter 46 [('lydia', 13), ('elizabeth', 12), ('go', 11), ('letter', 10), ('nothing', 10)]\n",
      "Chapter 47 [('elizabeth', 18), ('go', 18), ('well', 17), ('lydia', 15), ('jane', 14)]\n",
      "Chapter 48 [('gardiner', 11), ('bennet', 10), ('come', 9), ('letter', 9), ('go', 9)]\n",
      "Chapter 49 [('elizabeth', 13), ('come', 11), ('jane', 11), ('cry', 10), ('married', 10)]\n",
      "Chapter 50 [('bennet', 16), ('daughter', 11), ('wish', 10), ('lydia', 9), ('soon', 9)]\n",
      "Chapter 51 [('elizabeth', 13), ('lydia', 13), ('go', 13), ('sister', 10), ('wickham', 10)]\n",
      "Chapter 52 [('wickham', 13), ('uncle', 10), ('go', 10), ('come', 9), ('darcy', 9)]\n",
      "Chapter 53 [('come', 20), ('see', 17), ('elizabeth', 15), ('go', 14), ('bennet', 13)]\n",
      "Chapter 54 [('elizabeth', 12), ('well', 7), ('see', 7), ('come', 6), ('look', 6)]\n",
      "Chapter 55 [('elizabeth', 18), ('jane', 17), ('bennet', 15), ('go', 15), ('bingley', 13)]\n",
      "Chapter 56 [('elizabeth', 19), ('bennet', 17), ('lady', 15), ('catherine', 13), ('ladyship', 12)]\n",
      "Chapter 57 [('elizabeth', 11), ('lady', 8), ('catherine', 7), ('look', 7), ('letter', 7)]\n",
      "Chapter 58 [('elizabeth', 14), ('darcy', 8), ('think', 8), ('believe', 8), ('bingley', 7)]\n",
      "Chapter 59 [('lizzy', 16), ('darcy', 15), ('elizabeth', 13), ('bennet', 11), ('love', 9)]\n",
      "Chapter 60 [('elizabeth', 7), ('lady', 7), ('darcy', 6), ('love', 6), ('catherine', 6)]\n",
      "Chapter 61 [('project', 44), ('work', 42), ('gutenbergtm', 28), ('term', 19), ('electronic', 18)]\n"
     ]
    }
   ],
   "source": [
    "text = find_chapters(pride_lines[2229:])\n",
    "for chapter in ['Chapter {}'.format(x) for x in range(1, 62)]:\n",
    "    corpus = all_sentences(text[chapter])\n",
    "    print('{}'.format(chapter), get_frequecies(corpus, max_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
