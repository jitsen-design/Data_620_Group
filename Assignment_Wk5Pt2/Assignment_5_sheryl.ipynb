{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 620 Assignment: Document Classification\n",
    "\n",
    "Jithendra Seneviratne, Sheryl Piechocki \n",
    "\n",
    "July 3, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules and Libraries for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T15:51:52.082907Z",
     "start_time": "2020-06-30T15:51:52.078618Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text\n",
    "\n",
    "We'll be using text from a series of BBC articles found in [Kaggle](https://www.kaggle.com/yufengdev/bbc-fulltext-and-category?select=bbc-text.csv), categorized as one of the following:\n",
    "\n",
    "* Tech\n",
    "* Business\n",
    "* Sport\n",
    "* Entertainment\n",
    "* Politics\n",
    "\n",
    "The corpus consists of 2225 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:21:22.833310Z",
     "start_time": "2020-06-30T18:21:22.777776Z"
    }
   },
   "outputs": [],
   "source": [
    "text_df = pd.read_csv('bbc-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:21:24.527832Z",
     "start_time": "2020-06-30T18:21:24.523856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:21:26.614316Z",
     "start_time": "2020-06-30T18:21:26.606178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:22:28.299906Z",
     "start_time": "2020-06-30T18:22:28.283964Z"
    }
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') + ['mr',\n",
    "                                     'mrs',\n",
    "                                     'miss', \n",
    "                                     'say',\n",
    "                                     'have', \n",
    "                                     'might',\n",
    "                                     'thought',\n",
    "                                     'would', \n",
    "                                     'could', \n",
    "                                     'make', \n",
    "                                     'much',\n",
    "                                     'dear',\n",
    "                                     'must',\n",
    "                                     'know',\n",
    "                                     'one',\n",
    "                                     'good',\n",
    "                                     'every',\n",
    "                                     'towards',\n",
    "                                     'give',\n",
    "                                     'dr',\n",
    "                                     'none',\n",
    "                                     'go',\n",
    "                                     'come',\n",
    "                                     'upon',\n",
    "                                     'get',\n",
    "                                     'see',\n",
    "                                     'like',\n",
    "                                     'appear',\n",
    "                                     'sometimes',\n",
    "                                     'the',\n",
    "                                     'and',\n",
    "                                     'a',\n",
    "                                     'be',\n",
    "                                     'i',\n",
    "                                     'of',\n",
    "                                     'to',\n",
    "                                     'have',\n",
    "                                     'in',\n",
    "                                     'he',\n",
    "                                     'that',\n",
    "                                     'you',\n",
    "                                     'it',\n",
    "                                     'his',\n",
    "                                     'my',\n",
    "                                     'with',\n",
    "                                     'for',\n",
    "                                     'on',\n",
    "                                     'say',\n",
    "                                     'but',\n",
    "                                     'me',\n",
    "                                     'at',\n",
    "                                     'we',\n",
    "                                     'all',\n",
    "                                     'not',\n",
    "                                     'this',\n",
    "                                     'by',\n",
    "                                     'him',\n",
    "                                     'one',\n",
    "                                     'there',\n",
    "                                     'now',\n",
    "                                     'man',\n",
    "                                     'so',\n",
    "                                     'do',\n",
    "                                     'out',\n",
    "                                     'they',\n",
    "                                     'go',\n",
    "                                     'well',\n",
    "                                     'from',\n",
    "                                     'come',\n",
    "                                     'if',\n",
    "                                     'like',\n",
    "                                     'up',\n",
    "                                     'see',\n",
    "                                     'no',\n",
    "                                     'when',\n",
    "                                     'put',\n",
    "                                     'take',\n",
    "                                     'begin',\n",
    "                                     'two',\n",
    "                                     'three',\n",
    "                                     'u',\n",
    "                                     'still',\n",
    "                                     'last',\n",
    "                                     'never',\n",
    "                                     'always',\n",
    "                                     'thing',\n",
    "                                     'tell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Clean and Lemmatize Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:22:29.440903Z",
     "start_time": "2020-06-30T18:22:29.431912Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatize_word(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        tag = get_wordnet_pos(pos_tag([word])[0][1])\n",
    "        return lemmatizer.lemmatize(word, pos=tag)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def clean_doc(doc):\n",
    "\n",
    "    line= re.sub('[%s]' % re.escape(string.punctuation), '', doc)\n",
    "    line = re.sub('[^a-zA-Z\\ ]', '', line)\n",
    "    line = line.lower()\n",
    "    line = line.split()\n",
    "    line = ' '.join([lemmatize_word(x) for x in line if lemmatize_word(x) not in stop])\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply cleaning and lemmatizing functions to corpus  \n",
    "We clean the corpus by removing punctuation, lemmatizing the words, and removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:25:19.290061Z",
     "start_time": "2020-06-30T18:22:32.602764Z"
    }
   },
   "outputs": [],
   "source": [
    "text_df['cleaned_text'] = text_df['text'].apply(lambda x: clean_doc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:21:26.614316Z",
     "start_time": "2020-06-30T18:21:26.606178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom bos left book alone former worldcom b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tiger wary farrell gamble leicester rush bid a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  tv future hand viewer home theatre system plas...  \n",
       "1  worldcom bos left book alone former worldcom b...  \n",
       "2  tiger wary farrell gamble leicester rush bid a...  \n",
       "3  yeading face newcastle fa cup premiership side...  \n",
       "4  ocean twelve raid box office ocean twelve crim...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to vectorize corpus using TFIDF  \n",
    "The term frequency-inverse document frequency measure places importance on terms that are more frequent in a document, but are not frequent in all documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:27:53.945216Z",
     "start_time": "2020-06-30T18:27:53.939821Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_text(df,\n",
    "                   maxdf=.5,\n",
    "                   mindf=5,\n",
    "                   ngram_range=(1, 1),\n",
    "                   stop_words=stop):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range, \n",
    "                                       max_df=maxdf, \n",
    "                                       min_df=mindf, \n",
    "                                       stop_words=stop_words)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(text_df['cleaned_text'])\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    df_transform = pd.DataFrame(tfidf.toarray())\n",
    "    df_transform.columns = tfidf_feature_names\n",
    "    df_transform['category'] = text_df['category']\n",
    "    \n",
    "    return df_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply transformation to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:27:56.129683Z",
     "start_time": "2020-06-30T18:27:55.469543Z"
    }
   },
   "outputs": [],
   "source": [
    "df_transform = vectorize_text(text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abc</th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abn</th>\n",
       "      <th>...</th>\n",
       "      <th>zach</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zeppelin</th>\n",
       "      <th>zero</th>\n",
       "      <th>zhang</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  aaron  abandon  abandonment  abbott  abc     abide   ability  able  \\\n",
       "0  0.0    0.0      0.0          0.0     0.0  0.0  0.056514  0.000000   0.0   \n",
       "1  0.0    0.0      0.0          0.0     0.0  0.0  0.000000  0.041475   0.0   \n",
       "2  0.0    0.0      0.0          0.0     0.0  0.0  0.000000  0.000000   0.0   \n",
       "3  0.0    0.0      0.0          0.0     0.0  0.0  0.000000  0.000000   0.0   \n",
       "4  0.0    0.0      0.0          0.0     0.0  0.0  0.000000  0.000000   0.0   \n",
       "\n",
       "   abn  ...  zach  zealand  zeppelin  zero  zhang  zimbabwe  zombie  zone  \\\n",
       "0  0.0  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   0.0   \n",
       "1  0.0  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   0.0   \n",
       "2  0.0  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   0.0   \n",
       "3  0.0  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   0.0   \n",
       "4  0.0  ...   0.0      0.0       0.0   0.0    0.0       0.0     0.0   0.0   \n",
       "\n",
       "   zoom  zurich  \n",
       "0   0.0     0.0  \n",
       "1   0.0     0.0  \n",
       "2   0.0     0.0  \n",
       "3   0.0     0.0  \n",
       "4   0.0     0.0  \n",
       "\n",
       "[5 rows x 7193 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transform.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create logistic classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:28:54.731342Z",
     "start_time": "2020-06-30T18:28:54.725313Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_classifier(df):\n",
    "    \n",
    "    X = df.drop(labels=['category'],\n",
    "                axis=1)\n",
    "        \n",
    "    y = df['category']\n",
    "    lr = LogisticRegression(penalty='l2',\n",
    "                            dual=False,\n",
    "                            tol=.0001,\n",
    "                            C=1,\n",
    "                            )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=.3)\n",
    "\n",
    "    lr.fit(X_train,y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    print('Test Accuracy','{:.1%}'.format(lr.score(X_test, y_test)))\n",
    "    print('Train Accuracy','{:.1%}'.format(lr.score(X_train, y_train)))\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:29:12.998969Z",
     "start_time": "2020-06-30T18:29:10.947944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 96.4%\n",
      "Train Accuracy 99.8%\n",
      "Confusion Matrix\n",
      "[[151   0   6   0   1]\n",
      " [  1 114   2   0   0]\n",
      " [  1   2 116   2   0]\n",
      " [  0   0   0 159   0]\n",
      " [  1   3   1   4 104]]\n"
     ]
    }
   ],
   "source": [
    "logistic_classifier(df_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model seems to have done very well, classifying test data with high accuracy. Let's see if we can do better by changing the ngram range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change N-Gram Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:29:48.414059Z",
     "start_time": "2020-06-30T18:29:43.367841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 96.9%\n",
      "Train Accuracy 99.5%\n",
      "Confusion Matrix\n",
      "[[142   0   2   0   1]\n",
      " [  1 100   2   0   1]\n",
      " [  8   0 129   0   0]\n",
      " [  0   0   0 155   0]\n",
      " [  2   1   1   2 121]]\n"
     ]
    }
   ],
   "source": [
    "df_transform = vectorize_text(text_df,\n",
    "                              ngram_range=(1, 2))\n",
    "logistic_classifier(df_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, changing the number of ngrams from a max of one word to two words has helped let's try changing it to three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:45:01.401206Z",
     "start_time": "2020-06-30T18:44:54.718928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 97.5%\n",
      "Train Accuracy 99.6%\n",
      "Confusion Matrix\n",
      "[[164   0   1   1   3]\n",
      " [  2 116   1   0   0]\n",
      " [  4   0 109   0   2]\n",
      " [  0   0   0 142   0]\n",
      " [  1   1   1   0 120]]\n"
     ]
    }
   ],
   "source": [
    "df_transform = vectorize_text(text_df,\n",
    "                              ngram_range=(1, 3))\n",
    "logistic_classifier(df_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, now the model is overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda0f2540ca00c14c548fd578f4a6b1a719"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
