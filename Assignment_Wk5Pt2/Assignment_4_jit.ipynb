{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules and Libraries for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T15:51:52.082907Z",
     "start_time": "2020-06-30T15:51:52.078618Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text\n",
    "\n",
    "We'll be using text from a series of BBC articles found in [Kaggle](https://www.kaggle.com/yufengdev/bbc-fulltext-and-category?select=bbc-text.csv), categorized as one of the following:\n",
    "\n",
    "* Tech\n",
    "* Business\n",
    "* Sport\n",
    "* Entertainment\n",
    "* Politics\n",
    "\n",
    "The corpus consists of 2225 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:21:22.833310Z",
     "start_time": "2020-06-30T18:21:22.777776Z"
    }
   },
   "outputs": [],
   "source": [
    "text_df = pd.read_csv('bbc-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:21:24.527832Z",
     "start_time": "2020-06-30T18:21:24.523856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:21:26.614316Z",
     "start_time": "2020-06-30T18:21:26.606178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:22:28.299906Z",
     "start_time": "2020-06-30T18:22:28.283964Z"
    }
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') + ['mr',\n",
    "                                     'mrs',\n",
    "                                     'miss', \n",
    "                                     'say',\n",
    "                                     'have', \n",
    "                                     'might',\n",
    "                                     'thought',\n",
    "                                     'would', \n",
    "                                     'could', \n",
    "                                     'make', \n",
    "                                     'much',\n",
    "                                     'dear',\n",
    "                                     'must',\n",
    "                                     'know',\n",
    "                                     'one',\n",
    "                                     'good',\n",
    "                                     'every',\n",
    "                                     'towards',\n",
    "                                     'give',\n",
    "                                     'dr',\n",
    "                                     'none',\n",
    "                                     'go',\n",
    "                                     'come',\n",
    "                                     'upon',\n",
    "                                     'get',\n",
    "                                     'see',\n",
    "                                     'like',\n",
    "                                     'appear',\n",
    "                                     'sometimes',\n",
    "                                     'the',\n",
    "                                     'and',\n",
    "                                     'a',\n",
    "                                     'be',\n",
    "                                     'i',\n",
    "                                     'of',\n",
    "                                     'to',\n",
    "                                     'have',\n",
    "                                     'in',\n",
    "                                     'he',\n",
    "                                     'that',\n",
    "                                     'you',\n",
    "                                     'it',\n",
    "                                     'his',\n",
    "                                     'my',\n",
    "                                     'with',\n",
    "                                     'for',\n",
    "                                     'on',\n",
    "                                     'say',\n",
    "                                     'but',\n",
    "                                     'me',\n",
    "                                     'at',\n",
    "                                     'we',\n",
    "                                     'all',\n",
    "                                     'not',\n",
    "                                     'this',\n",
    "                                     'by',\n",
    "                                     'him',\n",
    "                                     'one',\n",
    "                                     'there',\n",
    "                                     'now',\n",
    "                                     'man',\n",
    "                                     'so',\n",
    "                                     'do',\n",
    "                                     'out',\n",
    "                                     'they',\n",
    "                                     'go',\n",
    "                                     'well',\n",
    "                                     'from',\n",
    "                                     'come',\n",
    "                                     'if',\n",
    "                                     'like',\n",
    "                                     'up',\n",
    "                                     'see',\n",
    "                                     'no',\n",
    "                                     'when',\n",
    "                                     'put',\n",
    "                                     'take',\n",
    "                                     'begin',\n",
    "                                     'two',\n",
    "                                     'three',\n",
    "                                     'u',\n",
    "                                     'still',\n",
    "                                     'last',\n",
    "                                     'never',\n",
    "                                     'always',\n",
    "                                     'thing',\n",
    "                                     'tell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:22:29.440903Z",
     "start_time": "2020-06-30T18:22:29.431912Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatize_word(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    try:\n",
    "        tag = get_wordnet_pos(pos_tag([word])[0][1])\n",
    "        return lemmatizer.lemmatize(word, pos=tag)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def clean_doc(doc):\n",
    "\n",
    "    line= re.sub('[%s]' % re.escape(string.punctuation), '', doc)\n",
    "    line = re.sub('[^a-zA-Z\\ ]', '', line)\n",
    "    line = line.lower()\n",
    "    line = line.split()\n",
    "    line = ' '.join([lemmatize_word(x) for x in line if lemmatize_word(x) not in stop])\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply cleaning and lemmatizing functions to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:25:19.290061Z",
     "start_time": "2020-06-30T18:22:32.602764Z"
    }
   },
   "outputs": [],
   "source": [
    "text_df['cleaned_text'] = text_df['text'].apply(lambda x: clean_doc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to vectorize corpus using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:27:53.945216Z",
     "start_time": "2020-06-30T18:27:53.939821Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_text(df,\n",
    "                   maxdf=.5,\n",
    "                   mindf=5,\n",
    "                   ngram_range=(1, 1),\n",
    "                   stop_words=stop):\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range, \n",
    "                                       max_df=maxdf, \n",
    "                                       min_df=mindf, \n",
    "                                       stop_words=stop_words)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(text_df['cleaned_text'])\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    df_transform = pd.DataFrame(tfidf.toarray())\n",
    "    df_transform.columns = tfidf_feature_names\n",
    "    df_transform['category'] = text_df['category']\n",
    "    \n",
    "    return df_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply transformation to corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:27:56.129683Z",
     "start_time": "2020-06-30T18:27:55.469543Z"
    }
   },
   "outputs": [],
   "source": [
    "df_transform = vectorize_text(text_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:28:54.731342Z",
     "start_time": "2020-06-30T18:28:54.725313Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_classifier(df):\n",
    "    \n",
    "    X = df.drop(labels=['category'],\n",
    "                axis=1)\n",
    "        \n",
    "    y = df['category']\n",
    "    lr = LogisticRegression(penalty='l2',\n",
    "                            dual=False,\n",
    "                            tol=.0001,\n",
    "                            C=1,\n",
    "                            )\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=.3)\n",
    "\n",
    "    lr.fit(X_train,y_train)\n",
    "    y_pred = lr.predict(X_test)\n",
    "    print('Test Accuracy',lr.score(X_test, y_test))\n",
    "    print('Train Accuracy',lr.score(X_train, y_train))\n",
    "    print('Confusion Matrix')\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:29:12.998969Z",
     "start_time": "2020-06-30T18:29:10.947944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.9715568862275449\n",
      "Train Accuracy 0.9974309569685292\n",
      "Confusion Matrix\n",
      "[[144   1   3   0   0]\n",
      " [  2  98   0   0   0]\n",
      " [  3   0 131   2   1]\n",
      " [  1   0   0 148   0]\n",
      " [  1   2   1   2 128]]\n"
     ]
    }
   ],
   "source": [
    "logistic_classifier(df_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model seems to have done very well, classifying test data with high accuracy. Let's see if we can do better by changing the ngram range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change N-Gram Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:29:48.414059Z",
     "start_time": "2020-06-30T18:29:43.367841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.9805389221556886\n",
      "Train Accuracy 0.9961464354527938\n",
      "Confusion Matrix\n",
      "[[150   0   2   0   1]\n",
      " [  2 119   0   0   0]\n",
      " [  3   0 124   0   1]\n",
      " [  0   0   0 154   0]\n",
      " [  3   1   0   0 108]]\n"
     ]
    }
   ],
   "source": [
    "df_transform = vectorize_text(text_df,\n",
    "                              ngram_range=(1, 2))\n",
    "logistic_classifier(df_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, changing the number of ngrams from a max of one word to two words has helped let's try changing it to three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T18:45:01.401206Z",
     "start_time": "2020-06-30T18:44:54.718928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.9700598802395209\n",
      "Train Accuracy 0.9961464354527938\n",
      "Confusion Matrix\n",
      "[[137   0   6   1   2]\n",
      " [  1 117   2   0   1]\n",
      " [  1   1 117   0   1]\n",
      " [  0   0   0 154   0]\n",
      " [  2   1   0   1 123]]\n"
     ]
    }
   ],
   "source": [
    "df_transform = vectorize_text(text_df,\n",
    "                              ngram_range=(1, 3))\n",
    "logistic_classifier(df_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, now the model is overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
